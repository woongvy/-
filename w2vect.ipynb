{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 38760,
     "databundleVersionId": 4493939,
     "sourceType": "competition"
    },
    {
     "sourceId": 4474043,
     "sourceType": "datasetVersion",
     "datasetId": 2601572
    }
   ],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install polars\n",
    "\n",
    "import polars as pl\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train = pl.read_parquet('../input/otto-full-optimized-memory-footprint/train.parquet')\n",
    "test = pl.read_parquet('../input/otto-full-optimized-memory-footprint/test.parquet')"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-24T13:19:51.237663Z",
     "iopub.execute_input": "2023-12-24T13:19:51.238038Z",
     "iopub.status.idle": "2023-12-24T13:20:25.625637Z",
     "shell.execute_reply.started": "2023-12-24T13:19:51.238007Z",
     "shell.execute_reply": "2023-12-24T13:20:25.624696Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sentences_df = pl.concat([train, test]).groupby('session').agg(\n",
    "    pl.col('aid').alias('sentence')\n",
    ")\n",
    "sentences = sentences_df['sentence'].to_list()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-24T13:20:25.627657Z",
     "iopub.execute_input": "2023-12-24T13:20:25.628494Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "w2vec = Word2Vec(sentences=sentences, vector_size=50, epochs=5, sg=1, window=3, sample=1e-3, ns_exponent=1, min_count=1, workers=4)\n",
    "\n",
    "aid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\n",
    "index = AnnoyIndex(50, 'euclidean')\n",
    "\n",
    "for aid, idx in aid2idx.items():\n",
    "    index.add_item(idx, w2vec.wv.vectors[idx])\n",
    "    \n",
    "index.build(10)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "sample_sub = pd.read_csv('../input/otto-recommender-system//sample_submission.csv')\n",
    "\n",
    "session_types = ['clicks', 'carts', 'orders']\n",
    "test_session_AIDs = test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n",
    "test_session_types = test.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n",
    "\n",
    "labels = []\n",
    "\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "for AIDs, types in zip(test_session_AIDs, test_session_types):\n",
    "    if len(AIDs) >= 20:\n",
    "        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n",
    "        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n",
    "        aids_temp=defaultdict(lambda: 0)\n",
    "        for aid,w,t in zip(AIDs,weights,types): \n",
    "            aids_temp[aid]+= w * type_weight_multipliers[t]\n",
    "            \n",
    "        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n",
    "        labels.append(sorted_aids[:20])\n",
    "    else:\n",
    "        # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n",
    "        AIDs = list(dict.fromkeys(AIDs[::-1]))\n",
    "        \n",
    "        # let's grab the most recent aid\n",
    "        most_recent_aid = AIDs[0]\n",
    "        \n",
    "        # and look for some neighbors!\n",
    "        nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n",
    "                        \n",
    "        labels.append((AIDs+nns)[:20])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n",
    "\n",
    "predictions = pd.DataFrame(data={'session_type': test_session_AIDs.index, 'labels': labels_as_strings})\n",
    "\n",
    "prediction_dfs = []\n",
    "\n",
    "for st in session_types:\n",
    "    modified_predictions = predictions.copy()\n",
    "    modified_predictions.session_type = modified_predictions.session_type.astype('str') + f'_{st}'\n",
    "    prediction_dfs.append(modified_predictions)\n",
    "\n",
    "submission = pd.concat(prediction_dfs).reset_index(drop=True)\n",
    "submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
